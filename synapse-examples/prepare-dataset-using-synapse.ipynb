{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, LinkedService\n",
        "from azureml.widgets import RunDetails"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1655712440865
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synapse_linked = 'synw-dmpbackup-westeu-01p-linked'\n",
        "synapse_compute_name = 'cc-small'\n",
        "synapse_pool_name = 'test'"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1655712440929
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1655712442920
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve the link between your Azure Synapse Analytics workspace and your Azure Machine Learning workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for service in LinkedService.list(ws) : \n",
        "    print(f\"Service: {service}\")\n",
        "\n",
        "# Retrieve a known linked service\n",
        "linked_service = LinkedService.get(ws, synapse_linked)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Service: LinkedService(workspace=Workspace.create(name='mlw-dmpbackup-westeu-01p', subscription_id='25a89471-60b3-4c91-b44f-ca49f38e6137', resource_group='rg-dmpbackup-westeu-01p'), name=synw-dmpbackup-westeu-01p-linked, type=LinkedServiceLinkType.synapse, linked_service_resource_id=/subscriptions/25a89471-60b3-4c91-b44f-ca49f38e6137/resourceGroups/rg-dmpbackup-westeu-01p/providers/Microsoft.Synapse/workspaces/synw-dmpbackup-westeu-01p, system_assigned_identity_principal_id=4920cd8e-47e7-4407-8d4b-2dc91a133c2f\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1655712443786
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attach your Apache spark pool as a compute target for Azure Machine Learning"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import SynapseCompute, ComputeTarget\n",
        "\n",
        "\n",
        "attach_config = SynapseCompute.attach_configuration(\n",
        "        linked_service=linked_service,\n",
        "        type=\"SynapseSpark\",\n",
        "        pool_name=synapse_pool_name,\n",
        ")\n",
        "\n",
        "synapse_compute = ComputeTarget.attach(\n",
        "        workspace=ws,\n",
        "        name=synapse_compute_name,\n",
        "        attach_configuration=attach_config,\n",
        ")\n",
        "\n",
        "synapse_compute.wait_for_completion()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Provisioning operation finished, operation \"Succeeded\"\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1655712445439
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a SynapseSparkStep that uses the linked Apache Spark pool"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.environment import Environment\n",
        "from azureml.pipeline.steps import SynapseSparkStep\n",
        "\n",
        "env = Environment(name=\"myenv\")\n",
        "env.python.conda_dependencies.add_pip_package(\"azureml-core>=1.20.0\")\n",
        "\n",
        "step_1 = SynapseSparkStep(\n",
        "    name='synapse-spark',\n",
        "    file='prep-dataset-synapse.py',\n",
        "    source_directory=\"./code\", \n",
        "    arguments=[\"--hday\", '2021-12-01', '--out_dataset_name', 'one-user-dataset', '--out_dataset_desc', 'dataset with only one user'],\n",
        "    compute_target=synapse_compute_name,\n",
        "    driver_memory=\"7g\",\n",
        "    driver_cores=4,\n",
        "    executor_memory=\"7g\",\n",
        "    executor_cores=2,\n",
        "    num_executors=1,\n",
        "    environment=env,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "only conda_dependencies specified in environment will be used in Synapse Spark run.\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1655712446430
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[step_1])\n",
        "pipeline_run = pipeline.submit('synapse-pipeline', regenerate_outputs=True)\n",
        "RunDetails(pipeline_run).show()\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step synapse-spark [1d380f9b][214eaf06-2647-4ec6-9976-2e7017dc13f6], (This step will run and generate new outputs)\nSubmitted PipelineRun 31140fcd-ce8d-42b5-8ffe-14dcd7601baf\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/31140fcd-ce8d-42b5-8ffe-14dcd7601baf?wsid=/subscriptions/25a89471-60b3-4c91-b44f-ca49f38e6137/resourcegroups/rg-dmpbackup-westeu-01p/workspaces/mlw-dmpbackup-westeu-01p&tid=041d21aa-b4ab-4ad1-891d-62207b3367ef\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9ab7c88761a47719431d9398e91e639"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/31140fcd-ce8d-42b5-8ffe-14dcd7601baf?wsid=/subscriptions/25a89471-60b3-4c91-b44f-ca49f38e6137/resourcegroups/rg-dmpbackup-westeu-01p/workspaces/mlw-dmpbackup-westeu-01p&tid=041d21aa-b4ab-4ad1-891d-62207b3367ef\", \"run_id\": \"31140fcd-ce8d-42b5-8ffe-14dcd7601baf\", \"run_properties\": {\"run_id\": \"31140fcd-ce8d-42b5-8ffe-14dcd7601baf\", \"created_utc\": \"2022-06-20T08:07:29.843976Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\", \"azureml.continue_on_step_failure\": \"False\", \"azureml.continue_on_failed_optional_input\": \"True\", \"azureml.pipelineComponent\": \"pipelinerun\"}, \"tags\": {}, \"end_time_utc\": \"2022-06-20T08:17:38.732605Z\", \"status\": \"Failed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://stdmpbackupmlwwesteu01p.blob.core.windows.net/azureml/ExperimentRun/dcid.31140fcd-ce8d-42b5-8ffe-14dcd7601baf/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=sF4FxK7YaDWdKNWFgI2dgMulsPTDvxPaxeDcg8EL8ZY%3D&skoid=c1efd837-baab-4c88-a36e-3f214eaff049&sktid=041d21aa-b4ab-4ad1-891d-62207b3367ef&skt=2022-06-20T07%3A57%3A31Z&ske=2022-06-21T16%3A07%3A31Z&sks=b&skv=2019-07-07&st=2022-06-20T08%3A30%3A06Z&se=2022-06-20T16%3A40%3A06Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://stdmpbackupmlwwesteu01p.blob.core.windows.net/azureml/ExperimentRun/dcid.31140fcd-ce8d-42b5-8ffe-14dcd7601baf/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=x89UvoqMzLrSXmZnhbPGNnbv%2BqAczOnCX353QZ93XPk%3D&skoid=c1efd837-baab-4c88-a36e-3f214eaff049&sktid=041d21aa-b4ab-4ad1-891d-62207b3367ef&skt=2022-06-20T07%3A57%3A31Z&ske=2022-06-21T16%3A07%3A31Z&sks=b&skv=2019-07-07&st=2022-06-20T08%3A30%3A06Z&se=2022-06-20T16%3A40%3A06Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://stdmpbackupmlwwesteu01p.blob.core.windows.net/azureml/ExperimentRun/dcid.31140fcd-ce8d-42b5-8ffe-14dcd7601baf/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=DjJVBWHJ1P2F3s5aEJWx6Mr3bQuf2RvMq4ckWQ0Eb8M%3D&skoid=c1efd837-baab-4c88-a36e-3f214eaff049&sktid=041d21aa-b4ab-4ad1-891d-62207b3367ef&skt=2022-06-20T07%3A57%3A31Z&ske=2022-06-21T16%3A07%3A31Z&sks=b&skv=2019-07-07&st=2022-06-20T08%3A30%3A06Z&se=2022-06-20T16%3A40%3A06Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:10:08\", \"run_number\": \"1655712449\", \"run_queued_details\": {\"status\": \"Failed\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"\", \"name\": \"synapse-spark\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2022-06-20 08:07:31Z] Submitting 1 runs, first five are: 1d380f9b:44ee5122-8943-475d-b9b5-21463399b6c3\\n[2022-06-20 08:17:38Z] Execution of experiment failed, update experiment status and cancel running nodes.\\n\\nError occurred: See child run or execution logs for more details.\\n\", \"graph\": {\"datasource_nodes\": {}, \"module_nodes\": {\"1d380f9b\": {\"node_id\": \"1d380f9b\", \"name\": \"synapse-spark\", \"status\": \"NotStarted\"}}, \"edges\": [], \"child_runs\": [{\"run_id\": \"\", \"name\": \"synapse-spark\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.42.0\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PipelineRunId: 31140fcd-ce8d-42b5-8ffe-14dcd7601baf\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/31140fcd-ce8d-42b5-8ffe-14dcd7601baf?wsid=/subscriptions/25a89471-60b3-4c91-b44f-ca49f38e6137/resourcegroups/rg-dmpbackup-westeu-01p/workspaces/mlw-dmpbackup-westeu-01p&tid=041d21aa-b4ab-4ad1-891d-62207b3367ef\nPipelineRun Status: Running\n\n\n\n\nPipelineRun Execution Summary\n==============================\nPipelineRun Status: Failed\n"
        },
        {
          "output_type": "error",
          "ename": "ActivityFailedException",
          "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"See child run or execution logs for more details.\",\n        \"messageFormat\": \"Child run failed for UserError\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"environment\": \"westeurope\",\n    \"location\": \"westeurope\",\n    \"time\": \"2022-06-20T08:17:38.732622Z\",\n    \"componentName\": \"\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"See child run or execution logs for more details.\\\",\\n        \\\"messageFormat\\\": \\\"Child run failed for UserError\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"environment\\\": \\\"westeurope\\\",\\n    \\\"location\\\": \\\"westeurope\\\",\\n    \\\"time\\\": \\\"2022-06-20T08:17:38.732622Z\\\",\\n    \\\"componentName\\\": \\\"\\\"\\n}\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
            "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m pipeline_run \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39msubmit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynapse-pipeline\u001b[39m\u001b[38;5;124m'\u001b[39m, regenerate_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m RunDetails(pipeline_run)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mpipeline_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:353\u001b[0m, in \u001b[0;36mPipelineRun.wait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    351\u001b[0m error \u001b[38;5;241m=\u001b[39m final_details\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m raise_on_error:\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ActivityFailedException(error_details\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_output:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mprint\u001b[39m(final_details)\n",
            "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"See child run or execution logs for more details.\",\n        \"messageFormat\": \"Child run failed for UserError\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"environment\": \"westeurope\",\n    \"location\": \"westeurope\",\n    \"time\": \"2022-06-20T08:17:38.732622Z\",\n    \"componentName\": \"\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"See child run or execution logs for more details.\\\",\\n        \\\"messageFormat\\\": \\\"Child run failed for UserError\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"environment\\\": \\\"westeurope\\\",\\n    \\\"location\\\": \\\"westeurope\\\",\\n    \\\"time\\\": \\\"2022-06-20T08:17:38.732622Z\\\",\\n    \\\"componentName\\\": \\\"\\\"\\n}\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1655713060571
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another way to submit script"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import RunConfiguration\n",
        "from azureml.core import ScriptRunConfig \n",
        "\n",
        "from azureml.core.environment import CondaDependencies\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"azureml-core==1.20.0\")\n",
        "\n",
        "run_config = RunConfiguration(framework=\"pyspark\")\n",
        "run_config.target = synapse_compute_name\n",
        "\n",
        "run_config.spark.configuration[\"spark.driver.memory\"] = \"7g\" \n",
        "run_config.spark.configuration[\"spark.driver.cores\"] = 2 \n",
        "run_config.spark.configuration[\"spark.executor.memory\"] = \"7g\" \n",
        "run_config.spark.configuration[\"spark.executor.cores\"] = 1 \n",
        "run_config.spark.configuration[\"spark.executor.instances\"] = 1 \n",
        "\n",
        "run_config.environment.python.conda_dependencies = conda_dep\n",
        "\n",
        "script_run_config=ScriptRunConfig(\n",
        "    source_directory='./code',\n",
        "    script='prep-dataset-synapse.py',\n",
        "    arguments=[\"--hday\", '2021-12-01', '--out_dataset_name', 'one-user-dataset', '--out_dataset_desc', 'dataset with only one user'],\n",
        "    run_config=run_config,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1655713118516
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "exp = Experiment(workspace=ws, name=\"synapse-spark\") \n",
        "run = exp.submit(config=script_run_config) \n",
        "RunDetails(run).show()\n",
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c93d43bff884d0fadd0349ce432d9c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/synapse-spark_1655713119_115e0e07?wsid=/subscriptions/25a89471-60b3-4c91-b44f-ca49f38e6137/resourcegroups/rg-dmpbackup-westeu-01p/workspaces/mlw-dmpbackup-westeu-01p&tid=041d21aa-b4ab-4ad1-891d-62207b3367ef\", \"run_id\": \"synapse-spark_1655713119_115e0e07\", \"run_properties\": {\"run_id\": \"synapse-spark_1655713119_115e0e07\", \"created_utc\": \"2022-06-20T08:18:40.131306Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"synapsespark\", \"ContentSnapshotId\": \"8038428d-db4d-421a-ab02-e06f6261e9c3\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2022-06-20T08:26:13.266816Z\", \"status\": \"Failed\", \"log_files\": {\"logs/azureml/driver/stderr\": \"https://stdmpbackupmlwwesteu01p.blob.core.windows.net/azureml/ExperimentRun/dcid.synapse-spark_1655713119_115e0e07/logs/azureml/driver/stderr?sv=2019-07-07&sr=b&sig=CC9UfILGZq8IJxctyOP8eYU24lxFL3nkWUcQUxtGf8k%3D&skoid=c1efd837-baab-4c88-a36e-3f214eaff049&sktid=041d21aa-b4ab-4ad1-891d-62207b3367ef&skt=2022-06-20T07%3A57%3A31Z&ske=2022-06-21T16%3A07%3A31Z&sks=b&skv=2019-07-07&st=2022-06-20T08%3A30%3A26Z&se=2022-06-20T16%3A40%3A26Z&sp=r\", \"logs/azureml/driver/stdout\": \"https://stdmpbackupmlwwesteu01p.blob.core.windows.net/azureml/ExperimentRun/dcid.synapse-spark_1655713119_115e0e07/logs/azureml/driver/stdout?sv=2019-07-07&sr=b&sig=aSWJ%2B9D4rRyrNDkXaPdNBVtzB503fd1neoMrFpZbC1k%3D&skoid=c1efd837-baab-4c88-a36e-3f214eaff049&sktid=041d21aa-b4ab-4ad1-891d-62207b3367ef&skt=2022-06-20T07%3A57%3A31Z&ske=2022-06-21T16%3A07%3A31Z&sks=b&skv=2019-07-07&st=2022-06-20T08%3A30%3A26Z&se=2022-06-20T16%3A40%3A26Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/driver/stderr\", \"logs/azureml/driver/stdout\"]], \"run_duration\": \"0:07:33\", \"run_number\": \"1655713120\", \"run_queued_details\": {\"status\": \"Failed\", \"details\": null}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"SLF4J: Class path contains multiple SLF4J bindings.\\nSLF4J: Found binding in [jar:file:/usr/hdp/5.0-62565507/spark3/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\nSLF4J: Found binding in [jar:file:/usr/hdp/5.0-62565507/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\\n22/06/20 08:24:23 INFO SignalUtils: Registering signal handler for TERM\\n22/06/20 08:24:24 INFO SignalUtils: Registering signal handler for HUP\\n22/06/20 08:24:24 INFO SignalUtils: Registering signal handler for INT\\n22/06/20 08:24:25 INFO SecurityManager: Changing view acls to: trusted-service-user\\n22/06/20 08:24:25 INFO SecurityManager: Changing modify acls to: trusted-service-user\\n22/06/20 08:24:25 INFO SecurityManager: Changing view acls groups to: \\n22/06/20 08:24:25 INFO SecurityManager: Changing modify acls groups to: \\n22/06/20 08:24:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(trusted-service-user); groups with view permissions: Set(); users  with modify permissions: Set(trusted-service-user); groups with modify permissions: Set()\\n22/06/20 08:24:25 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1655713239708_0001_000001\\n22/06/20 08:24:26 INFO ApplicationMaster: Starting the user application in a separate Thread\\n22/06/20 08:24:26 INFO ApplicationMaster: Waiting for spark context initialization...\\n22/06/20 08:24:26 INFO PythonRunner$: Initialized PythonRunnerOutputStream plugin org.apache.spark.microsoft.tools.api.plugin.MSToolsPythonRunnerOutputStreamPlugin.\\n22/06/20 08:24:26 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties\\n22/06/20 08:24:26 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\\n22/06/20 08:24:26 INFO MetricsSystemImpl: azure-file-system metrics system started\\n22/06/20 08:24:34 INFO SparkContext: Running Spark version 3.1.2.5.0-62565507\\n22/06/20 08:24:35 INFO ResourceUtils: ==============================================================\\n22/06/20 08:24:35 INFO ResourceUtils: No custom resources configured for spark.driver.\\n22/06/20 08:24:35 INFO ResourceUtils: ==============================================================\\n22/06/20 08:24:35 INFO SparkContext: Submitted application: Azure ML Experiment\\n22/06/20 08:24:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 384, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 7168, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\\n22/06/20 08:24:35 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\\n22/06/20 08:24:35 INFO ResourceProfileManager: Added ResourceProfile id: 0\\n22/06/20 08:24:35 INFO SecurityManager: Changing view acls to: trusted-service-user\\n22/06/20 08:24:35 INFO SecurityManager: Changing modify acls to: trusted-service-user\\n22/06/20 08:24:35 INFO SecurityManager: Changing view acls groups to: \\n22/06/20 08:24:35 INFO SecurityManager: Changing modify acls groups to: \\n22/06/20 08:24:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(trusted-service-user); groups with view permissions: Set(); users  with modify permissions: Set(trusted-service-user); groups with modify permissions: Set()\\n22/06/20 08:24:36 INFO Utils: Successfully started service 'sparkDriver' on port 45487.\\n22/06/20 08:24:36 INFO SparkEnv: Registering MapOutputTracker\\n22/06/20 08:24:36 INFO SparkEnv: Registering BlockManagerMaster\\n22/06/20 08:24:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\\n22/06/20 08:24:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\\n22/06/20 08:24:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\\n22/06/20 08:24:36 INFO DiskBlockManager: Created local directory at /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1655713239708_0001/blockmgr-def85027-b8d1-4e87-b2f3-91458a9782a2\\n22/06/20 08:24:36 INFO MemoryStore: MemoryStore started with capacity 3.6 GiB\\n22/06/20 08:24:36 INFO SparkEnv: Registering OutputCommitCoordinator\\n22/06/20 08:24:36 INFO log: Logging initialized @26585ms to org.sparkproject.jetty.util.log.Slf4jLog\\n22/06/20 08:24:36 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07\\n22/06/20 08:24:37 INFO Server: Started @26883ms\\n22/06/20 08:24:37 INFO AbstractConnector: Started ServerConnector@3c79631e{HTTP/1.1, (http/1.1)}{0.0.0.0:45581}\\n22/06/20 08:24:37 INFO Utils: Successfully started service 'SparkUI' on port 45581.\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@725c5ded{/jobs,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5682ef61{/jobs/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3de43611{/jobs/job,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4228e3d0{/jobs/job/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5c369956{/stages,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6a4459b1{/stages/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7d0bdf13{/stages/stage,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4f092b5a{/stages/stage/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5deee40f{/stages/pool,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@73bef360{/stages/pool/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4ca17914{/storage,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5b2e650e{/storage/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@45f305fa{/storage/rdd,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4868f8ca{/storage/rdd/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7753ae30{/environment,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2fd7b409{/environment/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2d3e6b71{/executors,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@75673188{/executors/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@23f8d62a{/executors/threadDump,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@181b95ae{/executors/threadDump/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4bf41b51{/static,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@39ba871e{/,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@110235e7{/api,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /metrics: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@e0b5604{/metrics,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7ab9bd60{/jobs/job/kill,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4cf4ef9e{/stages/stage/kill,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://vm-b8a49317:45581\\n22/06/20 08:24:37 INFO YarnClusterScheduler: Created YarnClusterScheduler\\n22/06/20 08:24:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43597.\\n22/06/20 08:24:37 INFO NettyBlockTransferService: Server created on vm-b8a49317:43597\\n22/06/20 08:24:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\\n22/06/20 08:24:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, vm-b8a49317, 43597, None)\\n22/06/20 08:24:37 INFO BlockManagerMasterEndpoint: Registering block manager vm-b8a49317:43597 with 3.6 GiB RAM, BlockManagerId(driver, vm-b8a49317, 43597, None)\\n22/06/20 08:24:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, vm-b8a49317, 43597, None)\\n22/06/20 08:24:37 INFO BlockManager: external shuffle service port = 7337\\n22/06/20 08:24:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, vm-b8a49317, 43597, None)\\n22/06/20 08:24:37 INFO SparkObservabilityBus: SparkDiagnosticEmitter: ShoeboxEmitter initialized\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7d31773d{/metrics/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO ServerInfo: Adding filter to /metrics/prometheus: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:37 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@73b98a39{/metrics/prometheus,null,AVAILABLE,@Spark}\\n22/06/20 08:24:37 INFO SingleEventLogFileWriter: Logging events to wasbs://2dcb9a92-418e-4a3f-8c12-672910bc12e5@dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net/events/96/eventLogs/application_1655713239708_0001_1.inprogress\\n22/06/20 08:24:38 INFO EnhancementLiveStatusPlugin: Enhancement Live App Status Plugin initialization\\n22/06/20 08:24:38 INFO EnhancementLiveStatusPlugin: Live app enhancement was enabled\\n22/06/20 08:24:38 INFO EnhancementAppStatusListener: attach Enhancement AppStatus Listener on live application\\n22/06/20 08:24:38 INFO DataOperations: configuration: spark.data.maxRecords: 1000\\n22/06/20 08:24:39 INFO SparkContext: Registered live app status plugin org.apache.spark.ui.EnhancementLiveStatusPlugin\\n22/06/20 08:24:39 INFO SparkContext: Registered live app status plugin org.apache.spark.diagnostic.synapse.SparkDiagnosticPlugin\\n22/06/20 08:24:39 INFO RpcAppSparkContextServer: Opening remote SparkContext service at 10.14.224.146:18083, remoteSparkContext/remoteSparkContextEndpoint\\n22/06/20 08:24:39 INFO RemoteSparkContextServer: Opening remote SparkContext server\\n22/06/20 08:24:39 INFO SecurityManager: Changing view acls to: trusted-service-user\\n22/06/20 08:24:39 INFO SecurityManager: Changing modify acls to: trusted-service-user\\n22/06/20 08:24:39 INFO SecurityManager: Changing view acls groups to: \\n22/06/20 08:24:39 INFO SecurityManager: Changing modify acls groups to: \\n22/06/20 08:24:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(trusted-service-user); groups with view permissions: Set(); users  with modify permissions: Set(trusted-service-user); groups with modify permissions: Set()\\n22/06/20 08:24:39 INFO Utils: Successfully started service 'remoteSparkContext' on port 18083.\\n22/06/20 08:24:39 INFO RemoteSparkContextServer: Will serve remote SparkContext on 10.14.224.146:18083, remoteSparkContext/remoteSparkContextEndpoint\\n22/06/20 08:24:39 INFO RpcAppListener: Got host of RPC history server from node info: vm-e7c41480\\n22/06/20 08:24:39 INFO RpcAppSender: Opening RPC app sender\\n22/06/20 08:24:39 INFO SecurityManager: Changing view acls to: trusted-service-user\\n22/06/20 08:24:39 INFO SecurityManager: Changing modify acls to: trusted-service-user\\n22/06/20 08:24:39 INFO SecurityManager: Changing view acls groups to: \\n22/06/20 08:24:39 INFO SecurityManager: Changing modify acls groups to: \\n22/06/20 08:24:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(trusted-service-user); groups with view permissions: Set(); users  with modify permissions: Set(trusted-service-user); groups with modify permissions: Set()\\n22/06/20 08:24:39 INFO TransportClientFactory: Successfully created connection to vm-e7c41480/10.14.224.147:18082 after 40 ms (0 ms spent in bootstraps)\\n22/06/20 08:24:40 INFO RpcAppSender: Will send events to RPC history server at vm-e7c41480:18082, rpcHistoryServer/rpcHistoryServerEndpoint\\n22/06/20 08:24:40 INFO SparkContext: Registered live app status plugin org.apache.spark.deploy.history.rpc.app.RpcAppLivePlugin\\n22/06/20 08:24:40 INFO EventsWriter$: Using the default value false for spark.peregrine.log.disableAnon\\n22/06/20 08:24:40 INFO EventsWriter$: Using the default value false for spark.peregrine.log.prodOverride\\n22/06/20 08:24:40 INFO SparkContext: Registered listener com.microsoft.hdinsight.spark.metrics.SparkMetricsListener\\n22/06/20 08:24:40 INFO SparkContext: Registered listener com.microsoft.peregrine.spark.listeners.PeregrineListenerSynapse\\n22/06/20 08:24:40 INFO SparkContext: Registered listener org.apache.spark.listeners.LogAnalyticsSparkListener\\n22/06/20 08:24:40 INFO SparkContext: Registered listener com.microsoft.autotune.listener.AutotuneApplicationListener\\n22/06/20 08:24:40 INFO RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\\n22/06/20 08:24:40 INFO YarnRMClient: Registering the ApplicationMaster\\n22/06/20 08:24:40 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...\\n22/06/20 08:24:41 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm2]\\n22/06/20 08:24:41 INFO ApplicationMaster: Preparing Local resources\\n22/06/20 08:24:41 INFO DefaultsConfigSparkListener: Persisted __spark_conf_merge_records__.json\\n22/06/20 08:24:41 INFO ApplicationMaster: \\n===============================================================================\\nDefault YARN executor launch context:\\n  env:\\n    CLASSPATH -> /usr/lib/library-manager/bin/libraries/scala/*<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>/opt/spark/jars/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\\n    SPARK_YARN_STAGING_DIR -> wasbs://2dcb9a92-418e-4a3f-8c12-672910bc12e5@dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net/user/trusted-service-user/trusted-service-user/.sparkStaging/application_1655713239708_0001\\n    PATH -> /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/usr/local/cuda-11.2/bin:/home/trusted-service-user/cluster-env/env/bin\\n    SPARK_USER -> trusted-service-user\\n    SPARK_HOME -> /opt/spark\\n    PYTHONPATH -> /opt/spark/python/lib/pyspark.zip<CPS>/opt/spark/python/lib/py4j-0.10.7-src.zip<CPS>{{PWD}}/source.zip<CPS>{{PWD}}/setup.zip\\n\\n  command:\\n    LD_LIBRARY_PATH=\\\\\\\"/usr/hdp/current/hadoop-client/lib/native:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/amd64/server:$LD_LIBRARY_PATH\\\\\\\" \\\\ \\n      {{JAVA_HOME}}/bin/java \\\\ \\n      -server \\\\ \\n      -Xmx7168m \\\\ \\n      '-Detwlogger.component=sparkexecutor' \\\\ \\n      '-DlogFilter.filename=SparkLogFilters.xml' \\\\ \\n      '-DpatternGroup.filename=SparkPatternGroups.xml' \\\\ \\n      '-Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer' \\\\ \\n      '-Dlog4jspark.log.dir=/var/log/sparkapp/\\\\${user.name}' \\\\ \\n      '-Dlog4jspark.log.file=sparkexecutor.log' \\\\ \\n      '-Dlog4j.configuration=file:/usr/hdp/current/spark3-client/conf/executor-log4j.properties' \\\\ \\n      '-Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl' \\\\ \\n      '-XX:+UseG1GC' \\\\ \\n      -Djava.io.tmpdir={{PWD}}/tmp \\\\ \\n      '-Dspark.driver.port=45487' \\\\ \\n      '-Dspark.synapse.history.rpc.port=18082' \\\\ \\n      '-Dspark.ui.port=0' \\\\ \\n      '-Dspark.history.ui.port=18080' \\\\ \\n      -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\\\ \\n      -XX:OnOutOfMemoryError='kill %p' \\\\ \\n      org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \\\\ \\n      --driver-url \\\\ \\n      spark://CoarseGrainedScheduler@vm-b8a49317:45487 \\\\ \\n      --executor-id \\\\ \\n      <executorId> \\\\ \\n      --hostname \\\\ \\n      <hostname> \\\\ \\n      --cores \\\\ \\n      1 \\\\ \\n      --app-id \\\\ \\n      application_1655713239708_0001 \\\\ \\n      --resourceProfileId \\\\ \\n      0 \\\\ \\n      --user-class-path \\\\ \\n      file:$PWD/__app__.jar \\\\ \\n      1><LOG_DIR>/stdout \\\\ \\n      2><LOG_DIR>/stderr\\n\\n  resources:\\n    setup.zip -> resource { scheme: \\\"wasbs\\\" host: \\\"dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net\\\" port: -1 file: \\\"/user/trusted-service-user/trusted-service-user/.sparkStaging/application_1655713239708_0001/setup.zip\\\" userInfo: \\\"2dcb9a92-418e-4a3f-8c12-672910bc12e5\\\" } size: 75080 timestamp: 1655713442000 type: FILE visibility: PRIVATE\\n    __spark_conf__ -> resource { scheme: \\\"wasbs\\\" host: \\\"dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net\\\" port: -1 file: \\\"/user/trusted-service-user/trusted-service-user/.sparkStaging/application_1655713239708_0001/__spark_conf__.zip\\\" userInfo: \\\"2dcb9a92-418e-4a3f-8c12-672910bc12e5\\\" } size: 305367 timestamp: 1655713444000 type: ARCHIVE visibility: PRIVATE\\n    source.zip -> resource { scheme: \\\"wasbs\\\" host: \\\"dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net\\\" port: -1 file: \\\"/user/trusted-service-user/trusted-service-user/.sparkStaging/application_1655713239708_0001/source.zip\\\" userInfo: \\\"2dcb9a92-418e-4a3f-8c12-672910bc12e5\\\" } size: 1015 timestamp: 1655713442000 type: FILE visibility: PRIVATE\\n\\n===============================================================================\\n22/06/20 08:24:41 INFO YarnAllocator: Executor Decommissioning Enabled\\n22/06/20 08:24:41 INFO YarnAllocator: Resource profile 0 doesn't exist, adding it\\n22/06/20 08:24:41 INFO RpcAppSender: Remote SparkContext server is opened on 10.14.224.146:18083, remoteSparkContext/remoteSparkContextEndpoint\\n22/06/20 08:24:41 INFO Configuration: resource-types.xml not found\\n22/06/20 08:24:41 INFO ResourceUtils: Unable to find 'resource-types.xml'.\\n22/06/20 08:24:41 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@vm-b8a49317:45487)\\n22/06/20 08:24:41 INFO YarnAllocator: Will request 1 executor container(s) for  ResourceProfile Id: 0, each with 1 core(s) and 7552 MB memory. with custom resources: <memory:7552, vCores:1>\\n22/06/20 08:24:41 INFO YarnAllocator: Submitted 1 unlocalized container requests.\\n22/06/20 08:24:41 INFO ApplicationMaster: Started progress reporter thread with (heartbeat : 1000, initial allocation : 200) intervals\\n22/06/20 08:24:41 INFO YarnAllocator: Launching container container_1655713239708_0001_01_000002 on host vm-b8a49317 for executor with ID 1 for ResourceProfile Id 0\\n22/06/20 08:24:41 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.\\n22/06/20 08:24:43 INFO RpcAppSender: Sent driver info of application_1655713239708_0001/1 to RPC history server\\n22/06/20 08:24:43 INFO AsyncEventQueue: Process of event SparkListenerApplicationStart(Azure ML Experiment,Some(application_1655713239708_0001),1655713474841,trusted-service-user,Some(1),Some(Map(stdout -> http://vm-b8a49317:8042/node/containerlogs/container_1655713239708_0001_01_000001/trusted-service-user/stdout?start=-4096, stderr -> http://vm-b8a49317:8042/node/containerlogs/container_1655713239708_0001_01_000001/trusted-service-user/stderr?start=-4096)),Some(Map(NM_HTTP_ADDRESS -> vm-b8a49317:8042, USER -> trusted-service-user, LOG_FILES -> stderr,stdout, NM_HTTP_PORT -> 8042, CLUSTER_ID -> ee150885-6193-4458-8316-e97b8cb6c4ca, NM_PORT -> 40385, HTTP_SCHEME -> http://, NM_HOST -> vm-b8a49317, CONTAINER_ID -> container_1655713239708_0001_01_000001))) by listener RpcAppListener took 2.494029422s.\\n22/06/20 08:24:45 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.14.224.146:43208) with ID 1,  ResourceProfileId 0\\n22/06/20 08:24:45 INFO YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\\n22/06/20 08:24:45 INFO YarnClusterScheduler: YarnClusterScheduler.postStartHook done\\n22/06/20 08:24:45 INFO SparkContext: Initialized SparkContextAfterInit plugin org.apache.spark.microsoft.tools.api.plugin.MSToolsSparkContextAfterInitPlugin.\\n22/06/20 08:24:45 INFO BlockManagerMasterEndpoint: Registering block manager vm-b8a49317:45893 with 4.0 GiB RAM, BlockManagerId(1, vm-b8a49317, 45893, None)\\n22/06/20 08:24:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('abfss://synapse@adlsdmpbcpsynwesteu01p.dfs.core.windows.net/synapse/workspaces/synw-dmpbackup-westeu-01p/warehouse') to the value of spark.sql.warehouse.dir ('abfss://synapse@adlsdmpbcpsynwesteu01p.dfs.core.windows.net/synapse/workspaces/synw-dmpbackup-westeu-01p/warehouse').\\n22/06/20 08:24:47 INFO SharedState: Warehouse path is 'abfss://synapse@adlsdmpbcpsynwesteu01p.dfs.core.windows.net/synapse/workspaces/synw-dmpbackup-westeu-01p/warehouse'.\\n22/06/20 08:24:47 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:47 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@355112fa{/SQL,null,AVAILABLE,@Spark}\\n22/06/20 08:24:47 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:47 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@257c5eea{/SQL/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:47 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:47 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@235a0d0d{/SQL/execution,null,AVAILABLE,@Spark}\\n22/06/20 08:24:47 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:47 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@62706f21{/SQL/execution/json,null,AVAILABLE,@Spark}\\n22/06/20 08:24:47 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/20 08:24:47 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4235fba3{/static/sql,null,AVAILABLE,@Spark}\\n22/06/20 08:24:49 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@17869a28)) by listener SparkMetricsListener took 2.085028257s.\\n22/06/20 08:24:50 INFO SessionTokenBasedTokenProvider: Setting up conf\\n22/06/20 08:24:50 INFO SessionTokenBasedTokenProvider: SessionTokenBasedTokenProvider initialized\\nANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.822/06/20 08:24:52 INFO SharedState: Creating database default if not exists\\n22/06/20 08:24:52 INFO HiveConf: Found configuration file file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/filecache/11/__spark_conf__.zip/__hadoop_conf__/hive-site.xml\\n22/06/20 08:24:52 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3 using file:/opt/hive-metastore/lib-2.3/hive-metastore-2.3.2.2.6.99.201-SNAPSHOT.jar:file:/opt/hive-metastore/lib-2.3/javax.jdo-3.2.0-m3.jar:file:/opt/hive-metastore/lib-2.3/hive-common-2.3.2.2.6.99.201-SNAPSHOT.jar:file:/opt/hive-metastore/lib-2.3/microsoft-catalog-metastore-client-1.0.59.jar:file:/opt/hive-metastore/lib-2.3/datanucleus-rdbms-4.1.19.jar:file:/opt/hive-metastore/lib-2.3/libfb303-0.9.3.jar:file:/opt/hive-metastore/lib-2.3/servlet-api-2.4.jar:file:/opt/hive-metastore/lib-2.3/mysql-connector-java-8.0.18.jar:file:/opt/hive-metastore/lib-2.3/commons-logging-1.2.jar:file:/opt/hive-metastore/lib-2.3/hive-exec-2.3.2.2.6.99.201-SNAPSHOT.jar:file:/opt/hive-metastore/lib-2.3/datanucleus-core-4.1.17.jar:file:/opt/hive-metastore/lib-2.3/datanucleus-api-jdo-4.2.4.jar:file:/opt/hive-metastore/lib-2.3/bonecp-0.8.0.RELEASE.jar:file:/opt/hive-metastore/lib-2.3/antlr-runtime-3.5.2.jar:file:/usr/hdp/current/hadoop-client/lib/woodstox-core-5.0.3.jar:file:/usr/hdp/current/hadoop-client/lib/kerby-asn1-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/gson-2.2.4.jar:file:/usr/hdp/current/hadoop-client/lib/stax2-api-3.1.4.jar:file:/usr/hdp/current/hadoop-client/lib/kerby-xdr-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/commons-cli-1.2.jar:file:/usr/hdp/current/hadoop-client/lib/commons-codec-1.11.jar:file:/usr/hdp/current/hadoop-client/lib/netty-3.10.5.Final.jar:file:/usr/hdp/current/hadoop-client/lib/json-smart-2.3.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-jaxrs-1.9.13.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-mapper-asl-1.9.13.jar:file:/usr/hdp/current/hadoop-client/lib/jul-to-slf4j-1.7.25.jar:file:/usr/hdp/current/hadoop-client/lib/json4s-scalap_2.12-3.7.0-M5.jar:file:/usr/hdp/current/hadoop-client/lib/jersey-core-1.19.jar:file:/usr/hdp/current/hadoop-client/lib/accessors-smart-1.2.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-server-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/commons-collections-3.2.2.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-databind-2.10.0.jar:file:/usr/hdp/current/hadoop-client/lib/json4s-jackson_2.12-3.7.0-M5.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-simplekdc-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/jsch-0.1.54.jar:file:/usr/hdp/current/hadoop-client/lib/flogger-system-backend-0.3.1.jar:file:/usr/hdp/current/hadoop-client/lib/trident-core-1.1.2.jar:file:/usr/hdp/current/hadoop-client/lib/lz4-1.2.0.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-server-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/slf4j-log4j12-1.7.25.jar:file:/usr/hdp/current/hadoop-client/lib/metrics-core-3.2.4.jar:file:/usr/hdp/current/hadoop-client/lib/aliyun-sdk-oss-2.8.3.jar:file:/usr/hdp/current/hadoop-client/lib/kerby-config-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/commons-lang-2.6.jar:file:/usr/hdp/current/hadoop-client/lib/commons-compress-1.4.1.jar:file:/usr/hdp/current/hadoop-client/lib/token-provider-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/curator-framework-2.12.0.jar:file:/usr/hdp/current/hadoop-client/lib/jersey-json-1.19.jar:file:/usr/hdp/current/hadoop-client/lib/commons-net-3.6.jar:file:/usr/hdp/current/hadoop-client/lib/kerby-util-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/json4s-ast_2.12-3.7.0-M5.jar:file:/usr/hdp/current/hadoop-client/lib/commons-configuration2-2.1.1.jar:file:/usr/hdp/current/hadoop-client/lib/guava-28.0-jre.jar:file:/usr/hdp/current/hadoop-client/lib/azure-storage-7.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/azure-data-lake-store-sdk-2.3.9.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-util-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/httpclient-4.5.2.jar:file:/usr/hdp/current/hadoop-client/lib/commons-beanutils-1.9.4.jar:file:/usr/hdp/current/hadoop-client/lib/commons-logging-1.1.3.jar:file:/usr/hdp/current/hadoop-client/lib/gcs-connector-hadoop3-1.9.10-shaded.jar:file:/usr/hdp/current/hadoop-client/lib/jaxb-api-2.2.11.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-webapp-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/jsp-api-2.1.jar:file:/usr/hdp/current/hadoop-client/lib/commons-lang3-3.4.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-identity-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/jersey-server-1.19.jar:file:/usr/hdp/current/hadoop-client/lib/commons-math3-3.1.1.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-core-asl-1.9.13.jar:file:/usr/hdp/current/hadoop-client/lib/javax.servlet-api-3.1.0.jar:file:/usr/hdp/current/hadoop-client/lib/zookeeper-3.4.6.5.0-62565507.jar:file:/usr/hdp/current/hadoop-client/lib/nimbus-jose-jwt-4.41.1.jar:file:/usr/hdp/current/hadoop-client/lib/snappy-java-1.0.5.jar:file:/usr/hdp/current/hadoop-client/lib/xz-1.0.jar:file:/usr/hdp/current/hadoop-client/lib/curator-recipes-2.12.0.jar:file:/usr/hdp/current/hadoop-client/lib/log4j-1.2.17.jar:file:/usr/hdp/current/hadoop-client/lib/hadoop-azure-trident-1.0.0.jar:file:/usr/hdp/current/hadoop-client/lib/curator-client-2.12.0.jar:file:/usr/hdp/current/hadoop-client/lib/tridenttokenlibrary-assembly-1.0.0.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-client-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/azure-keyvault-core-1.0.0.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-xc-1.9.13.jar:file:/usr/hdp/current/hadoop-client/lib/jdom-1.1.jar:file:/usr/hdp/current/hadoop-client/lib/httpcore-4.4.4.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-io-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-core-2.10.0.jar:file:/usr/hdp/current/hadoop-client/lib/jaxb-impl-2.2.3-1.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-core-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/slf4j-api-1.7.25.jar:file:/usr/hdp/current/hadoop-client/lib/kerby-pkix-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/json4s-core_2.12-3.7.0-M5.jar:file:/usr/hdp/current/hadoop-client/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:file:/usr/hdp/current/hadoop-client/lib/re2j-1.1.jar:file:/usr/hdp/current/hadoop-client/lib/htrace-core4-4.1.0-incubating.jar:file:/usr/hdp/current/hadoop-client/lib/wildfly-openssl-1.0.7.Final.jar:file:/usr/hdp/current/hadoop-client/lib/aws-java-sdk-bundle-1.11.375.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-admin-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/protobuf-java-2.5.0.jar:file:/usr/hdp/current/hadoop-client/lib/asm-5.0.4.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-security-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/animal-sniffer-annotations-1.17.jar:file:/usr/hdp/current/hadoop-client/lib/j2objc-annotations-1.3.jar:file:/usr/hdp/current/hadoop-client/lib/failureaccess-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/ojalgo-43.0.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-crypto-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-util-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-http-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-xml-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/jcip-annotations-1.0-1.jar:file:/usr/hdp/current/hadoop-client/lib/checker-qual-2.8.1.jar:file:/usr/hdp/current/hadoop-client/lib/TokenLibrary-assembly-3.3.1.jar:file:/usr/hdp/current/hadoop-client/lib/flogger-0.3.1.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-common-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-annotations-2.10.0.jar:file:/usr/hdp/current/hadoop-client/lib/jettison-1.1.jar:file:/usr/hdp/current/hadoop-client/lib/kafka-clients-0.8.2.1.jar:file:/usr/hdp/current/hadoop-client/lib/jersey-servlet-1.19.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-servlet-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/avro-1.7.7.jar:file:/usr/hdp/current/hadoop-client/lib/error_prone_annotations-2.3.2.jar:file:/usr/hdp/current/hadoop-client/lib/google-extensions-0.3.1.jar:file:/usr/hdp/current/hadoop-client/lib/commons-io-2.5.jar:file:/usr/hdp/current/hadoop-client/lib/flogger-log4j-backend-0.3.1.jar:file:/usr/hdp/current/hadoop-client/lib/jsr311-api-1.1.1.jar:file:/usr/hdp/current/hadoop-client/lib/jsr305-3.0.0.jar\\n22/06/20 08:24:52 INFO HiveConf: Found configuration file null\\n22/06/20 08:24:54 INFO SessionState: Created HDFS directory: wasbs://2dcb9a92-418e-4a3f-8c12-672910bc12e5@dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net/tmp/hive/trusted-service-user\\n22/06/20 08:24:54 INFO SessionState: Created local directory: /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1655713239708_0001/container_1655713239708_0001_01_000001/tmp/trusted-service-user\\n22/06/20 08:24:54 INFO SessionState: Created HDFS directory: wasbs://2dcb9a92-418e-4a3f-8c12-672910bc12e5@dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net/tmp/hive/trusted-service-user/97bcbc34-b79e-4b99-a981-374d902a684d\\n22/06/20 08:24:54 INFO SessionState: Created local directory: /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1655713239708_0001/container_1655713239708_0001_01_000001/tmp/trusted-service-user/97bcbc34-b79e-4b99-a981-374d902a684d\\n22/06/20 08:24:54 INFO SessionState: Created HDFS directory: wasbs://2dcb9a92-418e-4a3f-8c12-672910bc12e5@dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net/tmp/hive/trusted-service-user/97bcbc34-b79e-4b99-a981-374d902a684d/_tmp_space.db\\n22/06/20 08:24:54 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is abfss://synapse@adlsdmpbcpsynwesteu01p.dfs.core.windows.net/synapse/workspaces/synw-dmpbackup-westeu-01p/warehouse\\n22/06/20 08:24:55 INFO HiveMetastoreClientFactory: Checking hive.synapse.externalmetastore.linkedservice.name in Hive configuration = ls_mysql_hms\\n22/06/20 08:24:55 INFO MetastoreConfigurationProvider: getLinkedServiceName = ls_mysql_hms\\n22/06/20 08:24:56 WARN DefaultTimer: Can not service-load a timer. Using JavaTimer instead.\\n22/06/20 08:24:56 INFO TokenLibraryLinkedService: Attempting to fetch access token from cache\\n22/06/20 08:24:56 WARN InMemoryCacheClient: Token not found in in-memory cache\\n22/06/20 08:24:56 INFO TokenLibraryLinkedService: Invoking token service to fetch access token\\n22/06/20 08:24:57 INFO TokenLibraryLinkedService: Fetching access token\\n22/06/20 08:24:58 INFO finagle: Finagle version 19.5.1 (rev=21b8a8b3eeca571eedc7094df53d5eb806856b61) built at 20190520-194112\\n22/06/20 08:25:01 ERROR TokenLibraryLinkedService: POST failed {\\\"result\\\":\\\"DependencyError\\\",\\\"errorId\\\":\\\"InternalServerError\\\",\\\"errorMessage\\\":\\\"LSRServiceException is [{\\\\\\\"StatusCode\\\\\\\":401,\\\\\\\"ErrorResponse\\\\\\\":{\\\\\\\"code\\\\\\\":\\\\\\\"AccessControlUnauthorized\\\\\\\",\\\\\\\"message\\\\\\\":\\\\\\\"Insufficient permissions to call this API. 4920cd8e-47e7-4407-8d4b-2dc91a133c2f does not have Microsoft.Synapse/workspaces/read, Microsoft.Synapse/workspaces/linkedServices/useSecret/action on scope workspaces/synw-dmpbackup-westeu-01p/linkedServices/ls_mysql_hms\\\\\\\",\\\\\\\"target\\\\\\\":null},\\\\\\\"StackTrace\\\\\\\":\\\\\\\"   at Microsoft.Marlin.Common.ADF.Impl.LSRClient.CheckForFailures(HttpResponseMessage response, String responseContent) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\Common\\\\\\\\\\\\\\\\Microsoft.Marlin.Common.ADF\\\\\\\\\\\\\\\\Impl\\\\\\\\\\\\\\\\LSRClient.cs:line 348\\\\\\\\r\\\\\\\\n   at Microsoft.Marlin.Common.ADF.Impl.LSRClient.SendAsync(HttpRequestMessage request, CancellationToken cancellationToken, String traceId) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\Common\\\\\\\\\\\\\\\\Microsoft.Marlin.Common.ADF\\\\\\\\\\\\\\\\Impl\\\\\\\\\\\\\\\\LSRClient.cs:line 365\\\\\\\\r\\\\\\\\n   at Microsoft.Marlin.Common.ADF.Impl.LSRClient.ResolveLinkedServiceAsync(String linkedServiceName, ResolveAudienceRequest request, String traceId, CancellationToken cancellationToken) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\Common\\\\\\\\\\\\\\\\Microsoft.Marlin.Common.ADF\\\\\\\\\\\\\\\\Impl\\\\\\\\\\\\\\\\LSRClient.cs:line 202\\\\\\\\r\\\\\\\\n   at Microsoft.Marlin.TokenService.Token.LSRAudienceTokenProvider.GetToken(Boolean isLinkedService, String audience, String sessionToken, CancellationToken cancellationToken) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\TokenService\\\\\\\\\\\\\\\\Microsoft.Marlin.TokenService\\\\\\\\\\\\\\\\Token\\\\\\\\\\\\\\\\LSRAudienceTokenProvider.cs:line 153\\\\\\\\r\\\\\\\\n   at Microsoft.Marlin.TokenService.Token.LSRAudienceTokenProvider.GetTokenForAudienceAsync(Boolean isLinkedService, String audience, String account, String sessionToken, SignaturePayload signaturePayload, CancellationToken cancellationToken) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\TokenService\\\\\\\\\\\\\\\\Microsoft.Marlin.TokenService\\\\\\\\\\\\\\\\Token\\\\\\\\\\\\\\\\LSRAudienceTokenProvider.cs:line 127\\\\\\\\r\\\\\\\\n   at Microsoft.Marlin.TokenService.Controllers.TokenController.GetTokenAsync(TokenRequest request, CancellationToken cancellationToken) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\TokenService\\\\\\\\\\\\\\\\Microsoft.Marlin.TokenService\\\\\\\\\\\\\\\\Controllers\\\\\\\\\\\\\\\\TokenController.cs:line 82\\\\\\\\r\\\\\\\\n   at lambda_method3700(Closure , Object )\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ActionMethodExecutor.AwaitableObjectResultExecutor.Execute(IActionResultTypeMapper mapper, ObjectMethodExecutor executor, Object controller, Object[] arguments)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.<InvokeActionMethodAsync>g__Awaited|12_0(ControllerActionInvoker invoker, ValueTask`1 actionResultValueTask)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.<InvokeNextActionFilterAsync>g__Awaited|10_0(ControllerActionInvoker invoker, Task lastTask, State next, Scope scope, Object state, Boolean isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.Rethrow(ActionExecutedContextSealed context)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.Next(State& next, Scope& scope, Object& state, Boolean& isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.<InvokeInnerFilterAsync>g__Awaited|13_0(ControllerActionInvoker invoker, Task lastTask, State next, Scope scope, Object state, Boolean isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.<InvokeNextResourceFilter>g__Awaited|24_0(ResourceInvoker invoker, Task lastTask, State next, Scope scope, Object state, Boolean isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.Rethrow(ResourceExecutedContextSealed context)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.Next(State& next, Scope& scope, Object& state, Boolean& isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.<InvokeFilterPipelineAsync>g__Awaited|19_0(ResourceInvoker invoker, Task lastTask, State next, Scope scope, Object state, Boolean isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.<InvokeAsync>g__Awaited|17_0(ResourceInvoker invoker, Task task, IDisposable scope)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Routing.EndpointMiddleware.<Invoke>g__AwaitRequestTask|6_0(Endpoint endpoint, Task requestTask, ILogger logger)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Authorization.AuthorizationMiddleware.Invoke(HttpContext context)\\\\\\\\r\\\\\\\\n   at Swashbuckle.AspNetCore.SwaggerUI.SwaggerUIMiddleware.Invoke(HttpContext httpContext)\\\\\\\\r\\\\\\\\n   at Swashbuckle.AspNetCore.Swagger.SwaggerMiddleware.Invoke(HttpContext httpContext, ISwaggerProvider swaggerProvider)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Builder.Extensions.MapWhenMiddleware.Invoke(HttpContext context)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Diagnostics.ExceptionHandlerMiddleware.<Invoke>g__Awaited|6_0(ExceptionHandlerMiddleware middleware, HttpContext context, Task task)\\\\\\\",\\\\\\\"Message\\\\\\\":\\\\\\\"Insufficient permissions to call this API. 4920cd8e-47e7-4407-8d4b-2dc91a133c2f does not have Microsoft.Synapse/workspaces/read, Microsoft.Synapse/workspaces/linkedServices/useSecret/action on scope workspaces/synw-dmpbackup-westeu-01p/linkedServices/ls_mysql_hms\\\\\\\",\\\\\\\"Data\\\\\\\":{},\\\\\\\"InnerException\\\\\\\":null,\\\\\\\"HelpLink\\\\\\\":null,\\\\\\\"Source\\\\\\\":\\\\\\\"Microsoft.Marlin.Common.ADF\\\\\\\",\\\\\\\"HResult\\\\\\\":-2146233088}]. TraceId : ca0525e2-230c-4d97-8d2a-cc10c891917c. Error Component : LSR\\\"}\\n22/06/20 08:25:08 INFO finagle: FailureAccrualFactory marking connection to \\\"tokenservice1.westeurope.azuresynapse.net:443\\\" as dead. Remote Address: Inet(tokenservice1.westeurope.azuresynapse.net/10.250.0.7:443,Map())\\n22/06/20 08:25:15 INFO finagle: FailureAccrualFactory marking connection to \\\"tokenservice1.westeurope.azuresynapse.net:443\\\" as dead. Remote Address: Inet(tokenservice1.westeurope.azuresynapse.net/10.250.0.7:443,Map())\\n22/06/20 08:25:30 INFO finagle: FailureAccrualFactory marking connection to \\\"tokenservice1.westeurope.azuresynapse.net:443\\\" as dead. Remote Address: Inet(tokenservice1.westeurope.azuresynapse.net/10.250.0.7:443,Map())\\n22/06/20 08:25:50 INFO finagle: FailureAccrualFactory marking connection to \\\"tokenservice1.westeurope.azuresynapse.net:443\\\" as dead. Remote Address: Inet(tokenservice1.westeurope.azuresynapse.net/10.250.0.7:443,Map())\\n22/06/20 08:25:58 ERROR TokenLibraryLinkedService: POST failed\\ncom.twitter.util.TimeoutException: 1.minutes\\n\\tat com.twitter.util.Future.$anonfun$within$1(Future.scala:1642)\\n\\tat com.twitter.util.Future$$anon$4.apply$mcV$sp(Future.scala:1693)\\n\\tat com.twitter.util.Monitor.apply(Monitor.scala:46)\\n\\tat com.twitter.util.Monitor.apply$(Monitor.scala:41)\\n\\tat com.twitter.util.NullMonitor$.apply(Monitor.scala:229)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$2(Timer.scala:39)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.twitter.util.Local$.let(Local.scala:4904)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$1(Timer.scala:39)\\n\\tat com.twitter.util.Monitor.apply(Monitor.scala:46)\\n\\tat com.twitter.util.Monitor.apply$(Monitor.scala:41)\\n\\tat com.twitter.util.NullMonitor$.apply(Monitor.scala:229)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$2(Timer.scala:39)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.twitter.util.Local$.let(Local.scala:4904)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$1(Timer.scala:39)\\n\\tat com.twitter.util.JavaTimer.$anonfun$scheduleOnce$1(Timer.scala:233)\\n\\tat com.twitter.util.JavaTimer$$anon$3.run(Timer.scala:264)\\n\\tat java.util.TimerThread.mainLoop(Timer.java:555)\\n\\tat java.util.TimerThread.run(Timer.java:505)\\n22/06/20 08:25:58 ERROR MetastoreConfigurationProvider: Failed to get properties map of linked service [ls_mysql_hms] via TokenLibrary. Exception = {}\\ncom.twitter.util.TimeoutException: 1.minutes\\n\\tat com.twitter.util.Future.$anonfun$within$1(Future.scala:1642)\\n\\tat com.twitter.util.Future$$anon$4.apply$mcV$sp(Future.scala:1693)\\n\\tat com.twitter.util.Monitor.apply(Monitor.scala:46)\\n\\tat com.twitter.util.Monitor.apply$(Monitor.scala:41)\\n\\tat com.twitter.util.NullMonitor$.apply(Monitor.scala:229)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$2(Timer.scala:39)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.twitter.util.Local$.let(Local.scala:4904)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$1(Timer.scala:39)\\n\\tat com.twitter.util.Monitor.apply(Monitor.scala:46)\\n\\tat com.twitter.util.Monitor.apply$(Monitor.scala:41)\\n\\tat com.twitter.util.NullMonitor$.apply(Monitor.scala:229)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$2(Timer.scala:39)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.twitter.util.Local$.let(Local.scala:4904)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$1(Timer.scala:39)\\n\\tat com.twitter.util.JavaTimer.$anonfun$scheduleOnce$1(Timer.scala:233)\\n\\tat com.twitter.util.JavaTimer$$anon$3.run(Timer.scala:264)\\n\\tat java.util.TimerThread.mainLoop(Timer.java:555)\\n\\tat java.util.TimerThread.run(Timer.java:505)\\n22/06/20 08:25:58 WARN Hive: Failed to register all functions.\\norg.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Failed to get metastore properties from the linked service.)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3901)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:250)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:233)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:391)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:334)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:314)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:290)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.client(HiveClientImpl.scala:257)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:283)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:224)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:223)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:273)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:336)\\n\\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createDatabase$1(HiveExternalCatalog.scala:193)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\\n\\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:193)\\n\\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:137)\\n\\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:124)\\n\\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:153)\\n\\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:151)\\n\\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:60)\\n\\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:99)\\n\\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:99)\\n\\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupGlobalTempView(SessionCatalog.scala:870)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupTempView(Analyzer.scala:916)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupAndResolveTempView(Analyzer.scala:930)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$7.applyOrElse(Analyzer.scala:875)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$7.applyOrElse(Analyzer.scala:873)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.apply(Analyzer.scala:873)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1112)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1077)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\\n\\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\\n\\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\\n\\tat scala.collection.immutable.List.foldLeft(List.scala:89)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\\n\\tat scala.collection.immutable.List.foreach(List.scala:392)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:93)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:120)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:159)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:159)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:65)\\n\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\\n\\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:748)\\nCaused by: MetaException(message:Failed to get metastore properties from the linked service.)\\n\\tat com.microsoft.catalog.metastore.externalprovider.LinkedServiceMetastoreProvider.validateLinkedService(LinkedServiceMetastoreProvider.java:40)\\n\\tat com.microsoft.catalog.metastore.externalprovider.LinkedServiceMetastoreProvider.getAzureDatabaseLinkedService(LinkedServiceMetastoreProvider.java:104)\\n\\tat com.microsoft.catalog.metastore.externalprovider.LinkedServiceMetastoreProvider.init(LinkedServiceMetastoreProvider.java:34)\\n\\tat com.microsoft.catalog.metastore.externalprovider.MetastoreConfigurationProvider.applyTo(MetastoreConfigurationProvider.java:53)\\n\\tat com.microsoft.catalog.metastore.metastoreclient.HiveMetastoreClientFactory.createExternalHivemetasoreClient(HiveMetastoreClientFactory.java:85)\\n\\tat com.microsoft.catalog.metastore.metastoreclient.HiveMetastoreClientFactory.createMetaStoreClient(HiveMetastoreClientFactory.java:65)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3606)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3656)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3636)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3898)\\n\\t... 120 more\\n\\nError occurred: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Failed to get metastore properties from the linked service.)\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.42.0\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: synapse-spark_1655713119_115e0e07\nWeb View: https://ml.azure.com/runs/synapse-spark_1655713119_115e0e07?wsid=/subscriptions/25a89471-60b3-4c91-b44f-ca49f38e6137/resourcegroups/rg-dmpbackup-westeu-01p/workspaces/mlw-dmpbackup-westeu-01p&tid=041d21aa-b4ab-4ad1-891d-62207b3367ef\n\nExecution Summary\n=================\nRunId: synapse-spark_1655713119_115e0e07\nWeb View: https://ml.azure.com/runs/synapse-spark_1655713119_115e0e07?wsid=/subscriptions/25a89471-60b3-4c91-b44f-ca49f38e6137/resourcegroups/rg-dmpbackup-westeu-01p/workspaces/mlw-dmpbackup-westeu-01p&tid=041d21aa-b4ab-4ad1-891d-62207b3367ef\n\nWarnings:\nPython dependency specified in environment Conda dependencies is not supported in Synapse Spark pool. Synapse Spark pool now only supports fixed Python version, you can print \"sys.version_info\" in your script to check current Python version.\n{\n  \"error\": {\n    \"code\": \"SystemError\",\n    \"severity\": null,\n    \"message\": \"synw-dmpbackup-westeu-01p.test.96 Unexpected Synapse error during Synapse job run with error code LIVY_JOB_STATE_DEAD. Failed with message [plugins.synw-dmpbackup-westeu-01p.test.96 WorkspaceType:<Synapse> CCID:<de7f08bf-48a6-4eaa-a310-475091884102>] [Mon\",\n    \"messageFormat\": \"{Prefix} Unexpected Synapse error during Synapse job run with error code {ErrorCode}. Failed with message {Message}.\",\n    \"messageParameters\": {\n      \"Prefix\": \"synw-dmpbackup-westeu-01p.test.96\",\n      \"ErrorCode\": \"LIVY_JOB_STATE_DEAD\",\n      \"Message\": \"[plugins.synw-dmpbackup-westeu-01p.test.96 WorkspaceType:<Synapse> CCID:<de7f08bf-48a6-4eaa-a310-475091884102>] [Monitoring] Livy Endpoint=[https://hubservice1.westeurope.azuresynapse.net:8001/api/v1.0/publish/ee150885-6193-4458-8316-e97b8cb6c4ca]. Livy Id=[0]  Job failed during run time with state=[dead].\"\n    },\n    \"referenceCode\": null,\n    \"detailsUri\": null,\n    \"target\": null,\n    \"details\": [],\n    \"innerError\": {\n      \"code\": \"InternalError\",\n      \"innerError\": {\n        \"code\": \"SynapseServiceError\",\n        \"innerError\": {\n          \"code\": \"SynapseRuntimeError\",\n          \"innerError\": {\n            \"code\": \"OtherRuntimeError\",\n            \"innerError\": null\n          }\n        }\n      }\n    },\n    \"debugInfo\": null,\n    \"additionalInfo\": null\n  },\n  \"correlation\": null,\n  \"environment\": \"westeurope\",\n  \"location\": \"westeurope\",\n  \"time\": \"2022-06-20T08:26:13.1862215+00:00\",\n  \"componentName\": \"Execution\"\n}\n\n"
        },
        {
          "output_type": "error",
          "ename": "ActivityFailedException",
          "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Failed to get metastore properties from the linked service.)\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Failed to get metastore properties from the linked service.)\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
            "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m run \u001b[38;5;241m=\u001b[39m exp\u001b[38;5;241m.\u001b[39msubmit(config\u001b[38;5;241m=\u001b[39mscript_run_config) \n\u001b[1;32m      5\u001b[0m RunDetails(run)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/core/run.py:843\u001b[0m, in \u001b[0;36mRun.wait_for_completion\u001b[0;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_output:\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_run_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_handle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwait_post_processing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_post_processing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_details()\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/core/run.py:1096\u001b[0m, in \u001b[0;36mRun._stream_run_output\u001b[0;34m(self, file_handle, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ActivityFailedException(error_details\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   1098\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1099\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n",
            "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Failed to get metastore properties from the linked service.)\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Failed to get metastore properties from the linked service.)\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1655713608621
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}